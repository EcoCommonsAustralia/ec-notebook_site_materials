{
  "cells": [
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["![](https://raw.githubusercontent.com/EcoCommons-Australia-2024-2026/ec-notebook_site/main/images/notebooks_banner_withframe.png)\n\n# Species Distribution Modelling - Random Forest\n\nAuthor details: Xiang Zhao\n\nEditor details: Dr Jenna Wraith\n\nContact details: support\\@ecocommons.org.au\n\nCopyright statement: This script is the product of the EcoCommons platform. Please refer to the EcoCommons website for more details: https://www.ecocommons.org.au/\n\nDate: May 2025\n\n# Script information\n\nThis notebook, developed by the EcoCommons team, showcases how to build Species Distribution Models (SDMs) using Random Forest algorithm. It also compares the SDMs generated for the same species using Random Forest and Generalised Linear Models, highlighting their differences in predictive performance and spatial output.\n\n# Introduction\n\nUsing **Mountain Ash** (*Eucalyptus regnans*), a very tall and straight tree species native to Victoria and Tasmania, we will guide you through a standard protocol developed by Zurell et al. (2020) for building species distribution models (SDMs) with one of the most widely used algorithms: the **Random Forest (RF)**. We will also demonstrate the performance differences of SDMs produced by Random Forest algorithm and **Generalised Linear Model** (GLM) algorithm.\n\n# Key concepts\n\n## **K.1 Random forests**\n\nRandom Forests are an extension of single Classification Trees in which multiple decision trees are built with random subsets of the data. All random subsets have the same number of data points, and are selected from the complete dataset. Used data is placed back in the full dataset and can be selected in subsequent trees. In Random Forests, the random subsets are selected in a procedure called ‘bagging’, in which each data point has an equal probability of being selected for each new random subset. About two thirds of the total dataset is included in each random subset. The other third of the data is not used to build the trees, and this part is called the ‘out-of-the-bag’ data. This part is later used to evaluate the model.\n\n## **K.2 Decision tree**\n\nEach single classification tree is a decision tree. A decision tree is a predictive model that makes decisions by splitting data into branches based on **if-then rules**.\n\n![](https://raw.githubusercontent.com/EcoCommons-Australia-2024-2026/ec-notebook_site/main/images/Decision_tree.png)\n\nFor example, the above decision tree means:\n\n-   If **temperature \\> 15°C** and **precipitation \\> 1000 mm** then predict **species present**\n\n-   If **temperature \\> 15°C** but **precipitation ≤ 1000 mm** then predict **absent**\n\n-   If **temperature ≤ 15°C** and **elevation \\< 500 m** then predict **absent**\n\n-   If **temperature ≤ 15°C** and **elevation ≥ 500 m** then predict **present**\n\nThis is a **small decision tree** that splits the landscape into four groups using three environmental variables. A real random forest would grow many of these, each on a random subset of data and predictors, and combine their results.\n\n## **K.3 Bootstrap sampling and out-of-bag (OOB)**\n\nA bootstrap sampling means random sampling with replacement. When Random Forest builds each decision tree, it randomly selects a subset of the data from the original dataset. The subset is the same size as the original dataset. But some rows are repeated and some are left out.\n\nThe out-of-bag data are the observations that were not included in the bootstrap sample for a particular tree.\n\n![](https://raw.githubusercontent.com/EcoCommons-Australia-2024-2026/ec-notebook_site/main/images/Out_of_Bag_Set.png)\n\nFigure adapted from Smith et al. (2024) and Kwon and Zou (2023).\n\nRandom Forest uses these OOB samples as a built-in validation set: For each observation, it's passed down all the trees where it was OOB. The final OOB prediction is the majority vote (for classification) across those trees. On average, about 1/3 of the data is left our per tree for later in evaluation.\n\n![](https://raw.githubusercontent.com/EcoCommons-Australia-2024-2026/ec-notebook_site/main/images/randomforest_ecocommons.png)\n\n# Objectives\n\n1.  Familiarise yourself with the five main steps of running Random Forest for a tree species.\n2.  Learn how to adjust this Quarto Markdown notebook to run your own RF-based SDM.\n3.  Get accustomed to the style of the EcoCommons Notebooks.\n4.  Know the differences between GLM and RF.\n\n# **Workflow overview**\n\nFollowing a standard protocol for species distribution models proposed by Zurell et al., (2020), a R environment set-up step and five main modelling steps are demonstrated in this notebook:\n\n+--------------+--------------------------------+------------------------------------------------------------+\n| Step         | Main Step                      | Sub-steps                                                  |\n+==============+================================+============================================================+\n| Set-up       | Set-up                         | S.1 Set up the working directory                           |\n|              |                                |                                                            |\n|              |                                | S.2 Install and load required R packages for this notebook |\n|              |                                |                                                            |\n|              |                                | S.3 Download the case study datasets                       |\n+--------------+--------------------------------+------------------------------------------------------------+\n| Step 1       | Overview and Conceptualisation | 1.1 Taxon, location, predictors, scale                     |\n|              |                                |                                                            |\n|              | Data                           | 1.2 Biodiversity data                                      |\n|              |                                |                                                            |\n|              |                                | 1.3 Environmental data                                     |\n|              |                                |                                                            |\n|              |                                | 1.4 Combine data                                           |\n+--------------+--------------------------------+------------------------------------------------------------+\n| Step 2       | Model fitting                  | 2.1 Data splitting                                         |\n|              |                                |                                                            |\n|              |                                | 2.2 Model fitting                                          |\n+--------------+--------------------------------+------------------------------------------------------------+\n| Step 3       | Evaluation                     | 3.1 Model interpretation                                   |\n|              |                                |                                                            |\n|              |                                | 3.2 Cross-validation                                       |\n|              |                                |                                                            |\n|              |                                | 3.3 Mapping and Interpolation                              |\n+--------------+--------------------------------+------------------------------------------------------------+\n| Step 4       | Model comparison               | 4.1 Number of trees of Random Forest                       |\n|              |                                |                                                            |\n|              |                                | 4.2 Random Forest and GLM                                  |\n+--------------+--------------------------------+------------------------------------------------------------+\n\nIn the near future, this material may form part of comprehensive support materials available to EcoCommons users. If you have any corrections or suggestions to improve the efficiency, please [contact the EcoCommons](mailto:support@ecocommons.org.au) team.\n\n![](https://raw.githubusercontent.com/EcoCommons-Australia-2024-2026/ec-notebook_site/main/images/EC_breaker_nobackgoundcolor.png)\n\n# Set-up: R environment and packages\n\nSome housekeeping before we start. This process might take some time as many packages needed to be installed.\n\n## S.1 Set the working directory and create a folder for data.\n\nSave the Quarto Markdown file (.QMD) to a folder of your choice, and then set the path to your folder as your working directory.\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# Set the workspace to the current working directory\n# Uncomment and replace the path below with your own working directory if needed:\n# setwd(\"/Users/zhaoxiang/Documents/tmp/EC_RF_notebook\") \n\nworkspace <- getwd()  # Get the current working directory and store it in 'workspace'\n\n# Increase the plot size by adjusting the options for plot dimensions in the notebook output\noptions(repr.plot.width = 16, repr.plot.height = 8)  # Sets width to 16 and height to 8 for larger plots\n"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\n## S.2 Install and load essential libraries.\n\nInstall and load R packages.\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# Set CRAN mirror\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\"))\n\n# List of packages to check, install if needed, and load\npackages <- c(\"caret\", \"dplyr\", \"terra\", \"sf\", \"googledrive\", \"ggplot2\", \"corrplot\", \"pROC\", \"dismo\", \"spatstat.geom\", \"patchwork\", \"biomod2\", \"leaflet\", \"car\", \"gridExtra\", \"htmltools\", \"RColorBrewer\", \"knitr\", \"kableExtra\", \"randomForest\")\n\n# Function to display a cat message\ncat_message <- function(pkg, message_type) {\n  if (message_type == \"installed\") {\n    cat(paste0(pkg, \" has been installed successfully!\\n\"))\n  } else if (message_type == \"loading\") {\n    cat(paste0(pkg, \" is already installed and has been loaded!\\n\"))\n  }\n}\n\n# Install missing packages and load them\nfor (pkg in packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg)\n    cat_message(pkg, \"installed\")\n  } else {\n    cat_message(pkg, \"loading\")\n  }\n  library(pkg, character.only = TRUE)\n}"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\n## S.3 Download case study datasets\n\nWe have prepared the following data and uploaded them to our Google Drive for your use:\n\n-   **Species occurrence data:** Shapefile format (.shp)\n\n-   **Environmental variables:** Stacked Raster format (.tif)\n\n-   **Study area boundary:** Shapefile format (.shp)\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# De-authenticate Google Drive to access public files\ndrive_deauth()\n\n# Define Google Drive file ID and the path for downloading\nzip_file_id <- \"1wblHSFm1kkZLytNmpEBHnmEajR-bsA35\"\ndatafolder_path <- file.path(workspace)\n\n# Create a local path for the zipped file\nzip_file_path <- file.path(datafolder_path, \"mountain_ash_centralhighlands_data_RF.zip\")\n\n# Function to download a file with progress messages\ndownload_zip_file <- function(file_id, file_path) {\n  cat(\"Downloading zipped file...\\n\")\n  drive_download(as_id(file_id), path = file_path, overwrite = TRUE)\n  cat(\"Downloaded zipped file to:\", file_path, \"\\n\")\n}\n\n# Create local directory if it doesn't exist\nif (!dir.exists(datafolder_path)) {\n  dir.create(datafolder_path, recursive = TRUE)\n}\n\n# Download the zipped file\ncat(\"Starting to download the zipped file...\\n\")\ndownload_zip_file(zip_file_id, zip_file_path)\n\n# Unzip the downloaded file\ncat(\"Unzipping the file...\\n\")\nunzip(zip_file_path, exdir = datafolder_path)\ncat(\"Unzipped files to folder:\", datafolder_path, \"\\n\")\n"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\n# 1 Overview and Conceptualisation\n\n## 1.1 Taxon, location, and scale\n\n**Taxon:** Mountain Ash (*Eucalyptus regnans*)\n\n![](https://github.com/EcoCommons-Australia-2024-2026/ec-notebook_site_materials/raw/main/images/mountain_ash.jpeg){alt=\"Mountain Ash, \" width=\"674\"}\n\nPhotographer: [Reiner Richter, ALA](https://biocache.ala.org.au/occurrences/52a0b6eb-3656-4190-a8d6-ba4846b884a5)\n\n**Mountain Ash (*Eucalyptus regnans*)**, is a remarkably tall and straight tree native to Victoria and Tasmania. This species thrives in cool, temperate rainforest characterized by high rainfall, deep, well-drained soils, mild temperatures, and high humidity. It is typically found at altitudes ranging from 200 to 1,000 meters above sea level (Burns et al., 2015).\n\nThe Mountain Ash faces two main forms of disturbance: **bushfires**, which are its primary natural disturbance, and **logging**, which represents the primary human-induced threat to its habitat (Burns et al., 2015; Nevill et al., 2010).\n\n**Location:** **the Central Highlands (study area)** in the south part of Victoria\n\n**Spatial and temporal scales:** small (spatial) and static (temporal)\n\n## 1.2 Biodiversity data\n\nFor this exercise, we have prepared a species occurrence data file in CSV format, which was downloaded from ALA.\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\ncentral_highlands <- sf::st_read(\"mountain_ash_centralhighlands_RF/central_highlands.shp\")\n\nmountain_ash_centralhighlands <- sf::st_read(\"mountain_ash_centralhighlands_RF/mountain_ash_centralhighlands.shp\")\n\n# Filter the data to include only ABSENT points\nmountain_ash_present <- mountain_ash_centralhighlands %>%\n  dplyr::filter(occrrnS == \"1\")\n\n# Filter the data to include only ABSENT points\nmountain_ash_absent <- mountain_ash_centralhighlands %>%\n  dplyr::filter(occrrnS == \"0\")\n\nleaflet() %>%\n  addProviderTiles(providers$Esri.WorldImagery) %>%\n # Add the Central Highlands layer with a distinct color\n  addPolygons(data = central_highlands,\n              color = \"lightblue\",         # Border color of Central Highlands polygon\n              weight = 1,            # Border width\n              fillColor = \"lightblue\",  # Fill color of Central Highlands\n              fillOpacity = 0.3,     # Transparency for fill\n              group = \"Central Highlands\") %>%\n  \n    # Add Mountain Ash presence points\n  addCircleMarkers(data = mountain_ash_present,\n                   color = \"#11aa96\",\n                   radius = 1,\n                   weight = 0.5,\n                   opacity = 1,\n                   fillOpacity = 1,\n                   group = \"Mountain Ash Presence Records\") %>%\n  \n\n      # Add Mountain Ash absent points\n  addCircleMarkers(data = mountain_ash_absent,\n                   color = \"#f6aa70\",\n                   radius = 1,\n                   weight = 0.5,\n                   opacity = 1,\n                   fillOpacity = 1,\n                   group = \"Mountain Ash Pseudo-absent Records\") %>%\n  \n  setView(lng = 145.7, lat = -37.5, zoom = 7)  %>% # Adjust longitude, latitude, and zoom as needed \n  \n    # Add layer controls for easier toggling\n  addLayersControl(\n    overlayGroups = c(\"Central Highlands\", \"Mountain Ash Presence Records\", \"Mountain Ash Pseudo_absent Records\"),\n    options = layersControlOptions(collapsed = FALSE)\n  ) %>%\n  \n  # Add a legend for the layers\n  addControl(\n    html = \"\n    <div style='background-color: white; padding: 10px; border-radius: 5px;'>\n      <strong>Legend<\/strong><br>\n      <i style='background: lightblue; width: 18px; height: 18px; display: inline-block; margin-right: 8px; opacity: 0.7;'><\/i>\n      Central Highlands<br>\n      <i style='background: #11aa96; width: 10px; height: 10px; border-radius: 50%; display: inline-block; margin-right: 8px;'><\/i>\n      Mountain Ash Presence Records<br>\n      <i style='background: #f6aa70; width: 10px; height: 10px; border-radius: 50%; display: inline-block; margin-right: 8px;'><\/i>\n      Mountain Ash Pseudo-absent Records\n    <\/div>\n    \",\n    position = \"bottomright\"\n  )\n"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\n## 1.3 Environmental Data\n\n> 💡One core difference between how Random Forest and GLM learn from data is how they react to and deal with multicollinearity in independent variables.\n\nIn the [GLM notebook](https://github.com/ecocommonsaustralia/notebooks/blob/main/notebooks/EC_GLM.ipynb), when building SDM with Generalised Linear Model Algorithm, we used **correlation matrix** and **Variance Inflation Factor (VIF)** to detect correlation among environmental variables, so that we can select variables that are not highly correlated to avoid issue of multicollinearity. However, such treatment is not necessary for Random Forest. Because while GLM requires linearly independent predictors to ensure reliable inference, Random Forest doesn't try to separate the independent contribution of each variable. Which means, even if two variables are correlated, the Random Forest model remains stable and robust.\n\nTo ensure a valid comparison between RF and GLM, we will use the same set of eight environmental variables as in the [GLM notebook](https://github.com/ecocommonsaustralia/notebooks/blob/main/notebooks/EC_GLM.ipynb).\n\n+------------------------------------+----------------------------------------------------------------+-------------+--------------------------------------+\n| Environmental Variables Categories | Variables                                                      | Data Type   | Source                               |\n+====================================+================================================================+=============+======================================+\n| Temperature and Radiation          | Bioclim 04: Temperature Seasonality (standard deviation \\*100) | Continuous  | 1976-2005, CSIRO via EcoCommons      |\n|                                    |                                                                |             |                                      |\n|                                    | Bioclim 06: Min Temperature of Coldest Month                   |             |                                      |\n+------------------------------------+----------------------------------------------------------------+-------------+--------------------------------------+\n| Humidity                           | Bioclim 19: Precipitation of Coldest Quarter                   | Continuous  | 1976-2005, CSIRO via EcoCommons      |\n|                                    |                                                                |             |                                      |\n|                                    | Bioclim 35: Mean moisture index of coldest quarter             |             |                                      |\n+------------------------------------+----------------------------------------------------------------+-------------+--------------------------------------+\n| Topography                         | Digital Elevation Model                                        | Continuous  | Geoscience Australia via EcoCommons  |\n+------------------------------------+----------------------------------------------------------------+-------------+--------------------------------------+\n| Soil                               | Australian Soil Classification                                 | Categorical | Tern via EcoCommons                  |\n+------------------------------------+----------------------------------------------------------------+-------------+--------------------------------------+\n| Disturbance                        | Fires                                                          | Continuous  | Vic DEECA spatial data and resources |\n|                                    |                                                                |             |                                      |\n|                                    | Logging                                                        |             |                                      |\n+------------------------------------+----------------------------------------------------------------+-------------+--------------------------------------+\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# Load the stacked raster layers\nenv_var_stack <- rast(\"mountain_ash_centralhighlands_RF/env_stack.tif\")\n\n# We want to make sure that soil type raster layer is factor.\nenv_var_stack[[\"Soil_Type\"]] <- as.factor(env_var_stack[[\"Soil_Type\"]])\n\n# Check if the names are assigned correctly\nprint(names(env_var_stack))\n"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\n## 1.4 Combine species occurrence data and environmental variables\n\nWe will create a data frame that combines each presence/absence record of Mountain Ash with data from our 7 environmental variables.\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# Extract raster values at species occurrence points\nextracted_values  <- terra::extract(env_var_stack, vect(mountain_ash_centralhighlands))\n\n# Drop column of ID\nextracted_values <- extracted_values[, !(names(extracted_values) %in% \"ID\")]\n\n# Combine with species occurrence column\nmodel_data <- data.frame(occurrence = mountain_ash_centralhighlands$occrrnS, extracted_values)\n\n# Remove rows with any NA values in predictors\nmodel_data <- na.omit(model_data)\n\n# we want to make sure the data type of occurrence and soil types are factor.\nmodel_data$occurrence <- as.factor(model_data$occurrence)\nmodel_data$Soil_Type <- as.factor(model_data$Soil_Type)\n\nhead(model_data)"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\n# 2. Model fitting\n\n## 2.1 Data splitting\n\nFor cross-validation purposes, we need to leave out some data as testing dataset. There are many strategies of splitting data for cross-validation, like random, k-fold, and leave-one-out etc. Here we will use the easiest one: random splitting.\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# Set seed for reproducibility\nset.seed(123)\n\n# Split the data into training (80%) and testing (20%)\ntrain_index <- sample(1:nrow(model_data), size = 0.8 * nrow(model_data))\n\n# Create training and testing datasets\ntrain_data <- model_data[train_index, ]\ntest_data <- model_data[-train_index, ]\n\n# Check the split\ncat(\"Training Set:\", nrow(train_data), \"rows\\n\")\ncat(\"Testing Set:\", nrow(test_data), \"rows\\n\")\n"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\n## 2.2 Model fitting\n\n### 2.2.1 Random Forest\n\nWe use R package 'randomForest' to build our model.\n\n**'ntree'** sets the number of trees to build in the Random Forest. More trees usually have better performance and stability. However, it also means slower computation. Typical values of 'ntree' are 500 - 1000 (sometimes 100-200 is enough for fast testing).\n\n**'importance' is TRUE** tells the model to calculate variable importance metrics. This will give you which predictors are most important for explaining the response.\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# Fit the model\ntime_500tree <- system.time({rf_model <- randomForest(\n  occurrence ~ .,  # formula: species ~ all other variables\n  data = train_data,\n  ntree = 500,  # number of trees\n  importance = TRUE\n)\n}) # we can use system.time({}) to know how long does it take to run this model\n\n# Print model summary\nprint(rf_model)"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\n**Number of variables tried at each split (mtry):** 2\n\nAt each split, 2 predictors were randomly selected to choose the best split — this is the default for classification (√p where p = number of predictors).\n\n**Out-of-bag (OOB) error estimate:** 10.46%\n\nOn average, about **10.5% of samples were misclassified** when using only trees that did not see that sample in training (this is an internal cross-validation by Random Forest).\n\n**Confusion matrix**\n\n| Actual class | Predicted 0 | Predicted 1 | Class error |\n|----|----|----|----|\n| 0 (absence) | 989 | 187 | 15.9% error → 84.1% correctly predicted |\n| 1 (presence) | 142 | 1827 | 7.2% error → 92.8% correctly predicted |\n\nThe model is **stronger at predicting presence (1)** than absence (0). It **misclassifies absences more often**, which is common when presence points dominate or when background data is noisy.\n\n### 2.2.2 Generalised Linear Model\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# Fit the GLM on the training data\nglm_model <- glm(\n  occurrence ~ .,\n  data = train_data,\n  family = binomial(link = \"logit\")  # Logistic regression\n)"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\n# 3. Model evaluation\n\n## 3.1 Model interpretation\n\n### 3.1.1 Random Forest\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# Variable importance table\nvar_imp <- round(as.data.frame(importance(rf_model)), 2) #convert the Random Forest model importance matrix to a dataframe while rounds the numbers to two digits.\n\nkable(var_imp, caption = \"Random Forest Variable Importance\") %>%\n  kable_styling(font_size = 10) #Use knitr::kable() for a clean table\n"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\n**Mean Decrease Accuracy** and **Mean Decrease Gini** are two commonly used variable importance metrics for Random Forest.\n\n**Mean Decrease Accuracy** measures how much the model's prediction accuracy drops when a given variable's values are randomly permuted.\n\n**Mean Decrease Gini** measures the total decrease in Gini impurity (a measure of how pure a node is - the greater the decrease in Gini impurity, the more important the variable is considered) caused by splits on a variable, averages over all trees.\n\n✅ **Top overall predictors**\n\n-   Min_Temp_Coldest_Month → highest MeanDecreaseAccuracy (71.54) → strongest impact on improving predictive accuracy.\n\n-   Precip_Coldest_Quarter → second highest (70.40), especially important for predicting presences (1 = 56.94).\n\n-   Temp_Seasonality → third overall but highest MeanDecreaseGini (269.71) → very important for splitting decisions.\n\n✅ **Predictors more important for presences (1) than absences (0)**\n\n-   Precip_Coldest_Quarter (1 = 56.94 vs. 0 = 34.27)\n\n-   Min_Temp_Coldest_Month (1 = 52.59 vs. 0 = 49.96)\n\n-   Elevation (1 = 41.35 vs. 0 = 24.37)\n\n### 3.1.2 Generalised Linear Model {#generalised-linear-model-1}\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\nsummary(glm_model)\n"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\nFor the interpretation on GLM, please refer to <https://ecocommonsaustralia.github.io/notebook-blog/notebooks/EC_GLM/EC_GLM.html>\n\n## 3.2 Cross validation\n\nNow, we use the **testing data** to evaluate the model performance.\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# Predict probabilities and classes\nrf_probs <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]\nrf_preds <- factor(ifelse(rf_probs > 0.5, 1, 0), levels = c(0, 1))\n\n# AUC\nrf_roc <- roc(test_data$occurrence, rf_probs, quiet = TRUE)\nrf_auc <- auc(rf_roc)\nprint(rf_auc)\n\n# Confusion matrix\nrf_cm <- confusionMatrix(rf_preds, factor(test_data$occurrence, levels = c(0, 1)), positive = \"1\")\n\nprint(rf_cm)\n"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\nFrom above statistics, our model performs **very well overall**, especially in terms of:\n\n-   Discriminative power (AUC = 0.932)\n\n-   High accuracy (0.878) and specificity (0.937)\n\n-   Good sensitivity — although slightly lower, indicating the model misses some true 0s.\n\n-   Strong overall agreement (Kappa \\~0.73)\n\n-   Statistical significance across the board\n\nWe can also plot the AUC ROC Curve of our model.\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["# Plot ROC curve\nplot(rf_roc, col = \"#11aa96\", lwd = 1, main = \"AUC ROC Curve of the Random Forest model\")\n\n# Add AUC as text on the plot\nauc_value <- auc(rf_roc)\ntext(0.6, 0.2, paste(\"AUC =\", round(auc_value, 3)), col = \"#11aa96\")"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\n## 3.3 Mapping and Interpolation\n\nIf you remember, we started with these point records of mountain ash\n\n![](https://github.com/EcoCommons-Australia-2024-2026/ec-notebook_site_materials/raw/main/images/point_data.png){width=\"300px\" alt=\"Point data\"}\n\nNow, let's predict a continuous mapping of the distribution of mountain ash in the central highlands.\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# Predict the presence probability across the entire raster extent\nprediction_rf <- terra::predict(env_var_stack, rf_model, type = \"prob\", index = 2)\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# Define a custom color palette\ncustom_palette <- colorRampPalette(c(\"#f6aa70\", \"#61cafa\", \"#11aa96\"))\n\n# Plot the raster with the custom color palette\nplot(\n  prediction_rf,\n  main = \"Predicted Probability of the Presence of\\nMountain Ash in Central Highlands\\nwith Random Forest\",\n  col = custom_palette(100),\n  cex.main = 0.8\n)"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\nWe can also use an independent mountain ash data to cross-validate your mapping.\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\nforest_type_vic <- st_read(\"mountain_ash_centralhighlands_RF/FORTYPE500.shp\")\n\nforest_type_central_highlands <- st_intersection(forest_type_vic, central_highlands)\n\nggplot(data = forest_type_central_highlands) +\n  geom_sf(aes(fill = as.factor(X_DESC)), color = NA) +\n  scale_fill_manual(\n    name = \"Forest Type\",\n    values = c(\n      \"#D55E00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \n      \"#CC79A7\", \"#E69F00\", \"#000000\", \"#8E44AD\", \n      \"#3498DB\", \"#F39C12\",\"#27AE60\", \"#E74C3C\", \"#BDC3C7\"\n    )\n  ) +\n  labs(title = \"Forest Types in the Central Highlands\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    legend.text = element_text(size = 8),      # Smaller legend text\n    legend.title = element_text(size = 10),   # Smaller legend title\n    plot.title = element_text(size = 16, face = \"bold\"), # Larger plot title\n    axis.text = element_text(size = 10),      # Adjust axis text\n    axis.title = element_text(size = 12)      # Adjust axis title\n  ) +\n  coord_sf(expand = FALSE)\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\nmountain_ash_forest <- forest_type_central_highlands[forest_type_central_highlands$X_DESC == \"Mountain Ash\", ]\n\n# Plot the filtered shapefile\nggplot(data = mountain_ash_forest) +\n  geom_sf(fill = \"NA\", color = \"red\", size = 0.1) +  # Blue fill and black border\n  labs(\n    title = \"Mountain Ash Forest in Central Highlands\",\n    caption = \"Data Source: FORTYPE500.shp\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.text = element_text(size = 10),\n    axis.title = element_blank(),\n    legend.position = \"none\"  # No legend needed since this is a single type\n  )\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\ncustom_palette <- c(\"#f6aa70\", \"#61cafa\", \"#11aa96\")\n\n\npal <- colorNumeric(\n  palette = custom_palette,      # Use custom colors\n  domain = values(prediction_rf),\n  na.color = \"transparent\"       # Make NA values transparent\n)\n\n# Create the leaflet map\nleaflet() %>%\n  addProviderTiles(providers$Esri.WorldImagery) %>%\n  # Add raster layer\n  addRasterImage(prediction_rf, color = pal, opacity = 0.7, project = TRUE) %>%\n  # Add shapefile layer\n  addPolygons(\n    data = mountain_ash_forest,\n    color = \"red\",              # Border color of shapefile polygons\n    weight = 1,                  # Border width\n    fillColor = \"lightblue\",     # Fill color of shapefile polygons\n    fillOpacity = 0.3            # Transparency for fill\n  ) %>%\n  # Add legend for raster\n  addLegend(\n    pal = pal,\n    values = values(prediction_rf),\n    title = \"Probability\",\n    position = \"bottomright\"\n  )\n"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\n# 4. Model comparison\n\n## 4.1 Number of trees\n\nAs we mentioned in section 2.2.1, a higher number of trees generally leads to greater model accuracy but results in slower computation. Let's test this.\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# Fit the model with 100 trees\ntime_100tree <- system.time({\nrf_model_100tree <- randomForest(\n  occurrence ~ .,  # formula: species ~ all other variables\n  data = train_data,\n  ntree = 100,  # number of trees\n  importance = TRUE\n)\n})\n\n# Fit the model with 1000 trees\ntime_1000tree <- system.time({\nrf_model_1000tree <- randomForest(\n  occurrence ~ .,  # formula: species ~ all other variables\n  data = train_data,\n  ntree = 1000,  # number of trees\n  importance = TRUE\n)\n})\n"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\nFirst, let's compare the computational time of 100, 500, and 1000 trees.\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\ncat(\"100 trees elapsed time:\", time_100tree[\"elapsed\"], \"seconds\\n\")\ncat(\"500 trees elapsed time:\", time_500tree[\"elapsed\"], \"seconds\\n\")\ncat(\"1000 trees elapsed time:\", time_1000tree[\"elapsed\"], \"seconds\\n\")\n"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\nSo, more trees do take a longer time to finish.\n\nLet's see how the OOB error changes among models with different number of trees.\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\ncat(\"OOB Error (100 trees):\", rf_model_100tree$err.rate[nrow(rf_model_100tree$err.rate), \"OOB\"], \"\\n\")\ncat(\"OOB Error (500 trees):\", rf_model$err.rate[nrow(rf_model$err.rate), \"OOB\"], \"\\n\")\ncat(\"OOB Error (1000 trees):\", rf_model_1000tree$err.rate[nrow(rf_model_1000tree$err.rate), \"OOB\"], \"\\n\")"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\nThe OOB error decreased slightly as the number of trees increased, reaching the lowest point at 500 trees (10.59%). This suggests that 500 trees is likely sufficient for stable and accurate performance, as increasing to 1000 trees yields no further gain but increases computation time.\n\nNow, let's compare the AUC ROC curves.\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# Predict probabilities\npred_100 <- predict(rf_model_100tree, newdata = test_data, type = \"prob\")[,2]\npred_1000 <- predict(rf_model_1000tree, newdata = test_data, type = \"prob\")[,2]\n\n# Calculate ROC curves\nroc_100 <- roc(test_data$occurrence, pred_100, quiet = TRUE)\nroc_1000 <- roc(test_data$occurrence, pred_1000, quiet = TRUE)\n\n# Plot ROC curves\nplot(roc_100, col = \"lightblue\", lwd = 2, main = \"ROC Curves Comparison\", legacy.axes = TRUE)\nplot(rf_roc, col = \"#11aa96\", lwd = 2, add = TRUE)\nplot(roc_1000, col = \"#f6aa70\", lwd = 2, add = TRUE)\nabline(a = 0, b = 1, lty = 2, col = \"gray\")  # diagonal reference line\n\n# Add AUC legend\nlegend(\"bottomright\",\n       legend = c(\n         paste(\"100 trees (AUC =\", round(auc(roc_100), 3), \")\"),\n         paste(\"500 trees (AUC =\", round(auc(rf_roc), 3), \")\"),\n         paste(\"1000 trees (AUC =\", round(auc(roc_1000), 3), \")\")\n       ),\n       col = c(\"lightblue\", \"#11aa96\", \"#f6aa70\"),\n       lwd = 2,\n       cex = 0.5,         # Shrinks text size\n       inset = 0.02      # Pushes legend slightly inside the plot\n)"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\nIn this case, increasing the number of trees doesn't significantly improve the AUC of the model.\n\n## 4.2 Random Forest and GLM\n\nNow, let's compare the Random Forest model and GLM model.\n\nFirst, we need to compute the cross validation for GLM like we did for RF.\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# Predict probabilities and classes\nglm_probs <- predict(glm_model, newdata = test_data, type = \"response\")\nglm_preds <- factor(ifelse(glm_probs > 0.5, 1, 0), levels = c(0, 1))\n\n# AUC\nglm_roc <- roc(test_data$occurrence, glm_probs, quiet = TRUE)\nglm_auc <- auc(glm_roc)\n\n# Confusion matrix\nglm_cm <- confusionMatrix(glm_preds, factor(test_data$occurrence, levels = c(0, 1)), positive = \"1\")\n"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\n### 4.2.1 AUC-ROC curve\n\nLet's compare the AUC-ROC curve of each algorithm.\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["plot(rf_roc, col = \"#11aa96\", lwd = 2, main = \"RF vs. GLM ROC Curve\")\nplot(glm_roc, col = \"purple\", lwd = 2, add = TRUE)\nabline(a = 0, b = 1, lty = 2, col = \"gray\")\n\nlegend(\"bottomright\",\n       legend = c(paste(\"RF (AUC =\", round(auc(rf_roc), 3), \")\"),\n                  paste(\"GLM (AUC =\", round(auc(glm_roc), 3), \")\")),\n       col = c(\"#11aa96\", \"purple\"),\n       lwd = 2,\n       cex = 0.8,\n       bty = \"n\")"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\nThe ROC curve shows that the **Random Forest model outperforms the GLM model** in classifying the binary outcome. With an AUC of **0.932 vs 0.883**, RF achieves higher sensitivity and specificity across a range of thresholds, making it a more reliable model for this dataset.\n\n### 4.2.2 Importance of variables\n\nWe can also see how two algorithms rank the importance of variables.\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# Calculate Random Forest importance\nrf_importance <- importance(rf_model)\nrf_importance_df <- as.data.frame(rf_importance)\nrf_importance_df$Variable <- rownames(rf_importance_df)\n\n# Sort RF variables by importance\nrf_sorted <- rf_importance_df[order(-rf_importance_df$MeanDecreaseAccuracy), \"Variable\"]\n\n# Calculate GLM importance (absolute coefficients)\nglm_summary <- summary(glm_model)\nglm_coeffs <- glm_summary$coefficients\nglm_coeffs <- glm_coeffs[-1, , drop = FALSE]\n\nglm_importance_df <- data.frame(\n  Variable = rownames(glm_coeffs),\n  Coefficient = glm_coeffs[, \"Estimate\"],\n  Abs_Coefficient = abs(glm_coeffs[, \"Estimate\"])\n)\n\n# Sort GLM variables by absolute coefficient\nglm_sorted <- glm_importance_df[order(-glm_importance_df$Abs_Coefficient), \"Variable\"]\n\n# Get maximum length\nmax_len <- max(length(rf_sorted), length(glm_sorted))\n\n# Pad shorter list with NA\nrf_sorted <- c(rf_sorted, rep(NA, max_len - length(rf_sorted)))\nglm_sorted <- c(glm_sorted, rep(NA, max_len - length(glm_sorted)))\n\n# Create comparison table\ncomparison_table <- data.frame(\n  Rank = 1:max_len,\n  Random_Forest = rf_sorted,\n  GLM = glm_sorted\n)\n\nkable(comparison_table, caption = \"All Variables: Random Forest vs GLM (sorted by importance)\") %>% kable_styling(font_size = 10) #Use knitr::kable() for a clean table"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\nAs you can see in above table, these two algorithms rank the importance of variables differently.\n\n**Random Forest (RF)**\n\n-   RF ranks **climatic variables** like Min_Temp_Coldest_Month, Precip_Coldest_Quarter, and Temp_Seasonality as most important.\n\n-   It uses **numeric, continuous variables** effectively for splitting decision trees.\n\n-   **Categorical variables** like Soil_Type and Logging appear, but with less influence.\n\n**GLM**\n\n-   GLM relies heavily on **individual levels of categorical variables** (e.g., Soil_Type13, Soil_Type8, etc.).\n\n-   This is expected because GLMs break down categorical variables into **separate coefficients per level**.\n\n-   Some continuous variables like Moisture_Coldest_Quarter and Precip_Coldest_Quarter also contribute, but generally **less prominently**.\n\n> 💡 Another difference between Random Forest and GLM is how they deal with **individual levels of categorical variables**. GLM is a **parametric**, equation-based model. It needs **numeric inputs**, so it transforms categorical variables into **dummy (indicator) variables**. The model estimates separate **regression coefficients** for each level (relative to the reference). That's why you can see Soil_Type13, Soil_Type8, etc listed in the above table.\n>\n> RF is a **non-parametric, tree-based model**. It doesn’t need dummy variables — it can handle categorical variables **natively** (if they are encoded as factors in R). However, variable importance is reported **for the entire variable**, not per level. That's why you can only find Soil_Type in the above table instead of each levels of soil type.\n\n### 4.2.3 Performance statistics\n\nNow, let's compare the model statistics of two algorithms.\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# Extract stats\nextract_stats <- function(cm, auc_val) {\n  c(\n    Accuracy = cm$overall[\"Accuracy\"],\n    Kappa = cm$overall[\"Kappa\"],\n    Sensitivity = cm$byClass[\"Sensitivity\"],\n    Specificity = cm$byClass[\"Specificity\"],\n    Precision = cm$byClass[\"Pos Pred Value\"],\n    F1 = cm$byClass[\"F1\"],\n    AUC = auc_val\n  )\n}\n\nrf_stats <- extract_stats(rf_cm, rf_auc)\nglm_stats <- extract_stats(glm_cm, glm_auc)\n\n# Combine into a comparison table\ncomparison_table <- data.frame(\n  Metric = names(rf_stats),\n  Random_Forest = round(as.numeric(rf_stats), 4),\n  GLM = round(as.numeric(glm_stats), 4)\n)\n\n# Print result\nprint(comparison_table)\n"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\nThe **Random Forest model outperforms the GLM model across all major performance metrics**. It has higher accuracy, better class separation (AUC), stronger precision and recall balance (F1), and greater agreement with actual outcomes (Kappa). This suggests RF is a more robust classifier on this dataset, both in predictive power and error balance.\n\n### 4.2.4 Prediction map comparison\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# Predict the presence probability using the GLM model over the raster stack\nprediction_glm <- terra::predict(env_var_stack, glm_model, type = \"response\")\n\n# Calculate difference raster\nprediction_diff <- prediction_rf - prediction_glm\n"]
      ]
    },
    {
      "cell_type": ["code"],
      "metadata": [],
      "execution_count": {},
      "outputs": [],
      "source": [
        ["\n# Define custom color palette\ncustom_palette <- colorRampPalette(c(\"#f6aa70\", \"#61cafa\", \"#11aa96\"))\ndiff_palette <- colorRampPalette(c(\"red\", \"white\", \"blue\"))\n\n# 1. Random Forest prediction\nplot(\n  prediction_rf,\n  main = \"Random Forest Prediction\",\n  col = custom_palette(100),\n  cex.main = 1\n)\n\n# 2. GLM prediction\nplot(\n  prediction_glm,\n  main = \"GLM Prediction\",\n  col = custom_palette(100),\n  cex.main = 1\n)\n\n# 3. Difference map\nplot(\n  prediction_diff,\n  main = \"Difference (RF - GLM)\",\n  col = diff_palette(100),\n  cex.main = 1\n)"]
      ]
    },
    {
      "cell_type": ["markdown"],
      "metadata": [],
      "source": [
        ["\n**Interpretation of Color Scale**\n\n-   **Red areas** → **GLM predicts higher probabilities** than Random Forest (RF).\n\n-   **Blue areas** → **RF predicts higher probabilities** than GLM.\n\n-   **White areas** → Both models predict similar probabilities (difference near 0).\n\n**Spatial Insight**\n\n-   There is a **clear spatial structure** in model disagreement. Some regions are consistently blue or red, suggesting systematic differences in how each model interprets environmental features.\n\n**What to do next**\n\n-   Investigate **which environmental variables** contribute most to these differences.\n\n-   Consider plotting **variable importance** for both models to understand their decision mechanisms.\n\n-   Optionally, explore these differences using **residual maps**, **partial dependence plots**, or a **stacked ensemble model**.\n\n### 4.2.5 When to use what\n\n⚠️ Here comes the question: **if Random Forest model is always better, then when should we use GLM?**\n\nA short answer to this question is:\n\n> Use **GLM** when you need statistical interpretation and inference; use **Random Forest** when you prioritize prediction accuracy and can tolerate less interpretability.\n\nUse GLM when:\n\n> 1.  **Interpretability is important**\n>\n> GLM gives explicit coefficients for each predictor. Refering to [3.1.2 Generalised Linear Model](#generalised-linear-model-1), you get p-values, confidence intervals, and direction (positive/negative effect), which RF does not provide.\n>\n> 2.  **You need statistical inference**\n>\n> GLM gives you significance tests, effect size, and model diagnostics. However, RF is more like a black box - you don't get p-values or causal interpretation.\n>\n> 3.  **Your data is small**\n>\n> RF can **overfit** or behave unstably on small datasets. GLM is simpler, more stable, and works well with limited data.\n>\n> 4.  **Your variables are mostly linear and well-understood**\n>\n> If relationships are linear and the model is well specified, GLM can be nearly as accurate — and much easier to explain.\n\nUse RF when:\n\n> 1.  **You care more about prediction than interpretation**\n> 2.  **Data is large, complex, non-linear, or has interactions**\n> 3.  **You’re doing machine learning, not inference**\n> 4.  **You want to automate decisions, not explain them**\n\n# 5. Summary\n\nIn this notebook, we explored how the Random Forest algorithm works, introducing key concepts such as **decision tree**, **bootstrap sampling** and the **out-of-bag (OOB) validation** method.\n\nWe then talked through the complete process of building a Random Forest model and interpreting its performance using metrix like **Mean Decrease Accuracy** and **Mean Decrease Gini.**\n\nFinally, we compared Random Forest and Generalised Linear Model, highlighting key differences between in how each algorithm handles variable importance, multicollinearity and model interpretation:\n\n> 💡GLM is sensitive to multicollinearity and requires the removal of highly correlated predictors to ensure reliable inference, typically using tools like correlation matrices and VIF. In contrast, Random Forest is robust to multicollinearity because it doesn’t rely on estimating independent contributions of each variable, allowing correlated variables to coexist without degrading model performance.\n>\n> 💡GLM transforms categorical variables into dummy variables and estimates separate coefficients for each level, allowing detailed interpretation of individual categories. In contrast, Random Forest handles categorical variables natively and reports importance at the whole-variable level, not for individual levels.\n\nWe also discussed the advantages and disadvantages of using these two algorithms:\n\n> Use **GLM** when interpretability, statistical inference, or understanding variable effects is important—especially with small or well-structured data. It provides coefficients, p-values, and confidence intervals, making it ideal for transparent analysis.\n>\n> Use **Random Forest** when your goal is prediction over explanation, particularly with large, complex, or non-linear data. While less interpretable, it handles interactions and multicollinearity robustly and often achieves higher predictive accuracy.\n\n# References\n\nBreiman, L. (2001), *Random Forests*, Machine Learning 45(1), 5-32.\n\nBurns, E. L., Lindenmayer, D. B., Stein, J., Blanchard, W., McBurney, L., Blair, D., & Banks, S. C. (2015). Ecosystem assessment of mountain ash forest in the C entral H ighlands of V ictoria, south‐eastern A ustralia. *Austral Ecology*, *40*(4), 386-399.\n\nKwon, Y., & Zou, J. (2023, July). Data-oob: Out-of-bag estimate as a simple and efficient data value. In *International conference on machine learning* (pp. 18135-18152). PMLR.\n\nNevill, P. G., Bossinger, G., & Ades, P. K. (2010). Phylogeography of the world’s tallest angiosperm, Eucalyptus regnans: evidence for multiple isolated Quaternary refugia. *Journal of Biogeography*, *37*(1), 179-192.\n\nSmith, H. L., Biggs, P. J., French, N. P., Smith, A. N., & Marshall, J. C. (2024). Out of (the) bag—encoding categorical predictors impacts out-of-bag samples. *PeerJ Computer Science*, *10*, e2445. \n\nvon Takach Dukai, B. (2019). The genetic and demographic impacts of contemporary disturbance regimes in mountain ash forests.\n\nZurell, D., Franklin, J., König, C., Bouchet, P. J., Dormann, C. F., Elith, J., Fandos, G., Feng, X., Guillera‐Arroita, G., & Guisan, A. (2020). A standard protocol for reporting species distribution models. *Ecography*, *43*(9), 1261-1277.\n\nLogging History (1930-07-01 - 2022-06-30), Data VIC, 2024, <https://discover.data.vic.gov.au/dataset/logging-history-overlay-of-most-recent-harvesting-activities>\n\nFire History (2011-2021), Data VIC, 2024,  <https://www.agriculture.gov.au/abares/forestsaustralia/forest-data-maps-and-tools/spatial-data/forest-fire#fires-in-australias-forests-201116-2018>\n\nHarwood, Tom (2019): 9s climatology for continental Australia 1976-2005: BIOCLIM variable suite. v1. CSIRO. Data Collection. <https://doi.org/10.25919/5dce30cad79a8>\n\n3 second SRTM Derived Digital Elevation Model (DEM) Version 1.0, Geoscience Australia, 2018, <https://dev.ecat.ga.gov.au/geonetwork/srv/api/records/a05f7892-ef04-7506-e044-00144fdd4fa6>\n\nSearle, R. (2021): Australian Soil Classification Map. Version 1. Terrestrial Ecosystem Research Network. (Dataset). [**https://doi.org/10.25901/edyr-wg85**](https://doi.org/10.25901/edyr-wg85)\n\n![](https://raw.githubusercontent.com/EcoCommons-Australia-2024-2026/ec-notebook_site/main/images/EC_section_break.png)\n\nEcoCommons received investment (<https://doi.org/10.3565/chbq-mr75>) from the Australian Research Data Commons (ARDC). The ARDC is enabled by the National Collaborative Research Infrastructure Strategy (NCRIS).\n\n::: {align=\"center\"}\n**Our partner**\n:::\n\n![](https://raw.githubusercontent.com/EcoCommons-Australia-2024-2026/ec-notebook_site/main/images/partners_logos.png)\n\n# **How to Cite EcoCommons**\n\nIf you use EcoCommons in your research, please cite the platform as follows:\n\n> EcoCommons Australia 2024. *EcoCommons Australia – a collaborative commons for ecological and environmental modelling*, Queensland Cyber Infrastructure Foundation, Brisbane, Queensland. Available at: <https://data–explorer.app.ecocommons.org.au/> (Accessed: MM DD, YYYY). <https://doi.org/10.3565/chbq-mr75>\n\nYou can download the citation file for EcoCommons Australia here: [Download the BibTeX file](reference.bib)"]
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": ["python"]
    }
  },
  "nbformat": [4],
  "nbformat_minor": [2]
}
